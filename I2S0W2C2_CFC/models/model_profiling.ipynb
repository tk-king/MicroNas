{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28620e-e4cb-4b3e-8781-e1f09a7e5571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6504d-6bdf-4fd7-9064-dc553b250c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1da94-f884-4fe2-9eb0-5cb41bdc558a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b5da2-602e-48d6-ad3a-fa6e3d126d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99158b9-9309-4fb1-964b-cd803f9960f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ptflops import get_model_complexity_info\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from CrossAttn import CrossAttn\n",
    "from deepSense import DeepSense\n",
    "from GlobalFusion import GlobalFusion\n",
    "from ALAE import ALAE_TAE\n",
    "from Attend import AttendDiscriminate\n",
    "from deepconvlstm import DeepConvLSTM\n",
    "from ifconv import If_ConvTransformer_W\n",
    "from TinyHAR import TinyHAR_Model\n",
    "from MixerMLP import FFTMIXER_HAR_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c84a3d-0788-4724-8769-fc006f9bf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [\n",
    "    # {\"dataset_name\":\"pamap2\",  \"sensor_channel\":18, \"window_length\":200, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":18, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":2,\"L_sensor_locations\":3},#\n",
    "    # {\"dataset_name\":\"dsads\",  \"sensor_channel\":45, \"window_length\":125, #\n",
    "    #  \"fft_window_length\":5, \"nb_classes\":19, \"fft_feature_dim\":50, #\n",
    "    #  \"S_number_sensors_type\":3,\"L_sensor_locations\":5},#\n",
    "    # {\"dataset_name\":\"dg\",  \"sensor_channel\":9, \"window_length\":128, #\n",
    "    #  \"fft_window_length\":8, \"nb_classes\":2, \"fft_feature_dim\":32, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":3},#\n",
    "    # {\"dataset_name\":\"rw\",  \"sensor_channel\":45, \"window_length\":100, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":8, \"fft_feature_dim\":20, #\n",
    "    #  \"S_number_sensors_type\":3,\"L_sensor_locations\":5},#\n",
    "    # {\"dataset_name\":\"hapt\",  \"sensor_channel\":9, \"window_length\":128, #\n",
    "    #  \"fft_window_length\":8, \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "    #  \"S_number_sensors_type\":3,\"L_sensor_locations\":1},#\n",
    "    # {\"dataset_name\":\"motionsense\",  \"sensor_channel\":12, \"window_length\":128, #\n",
    "    #  \"fft_window_length\":8, \"nb_classes\":6, \"fft_feature_dim\":32, \n",
    "    #  \"S_number_sensors_type\":4,\"L_sensor_locations\":1},#\n",
    "    # {\"dataset_name\":\"skoda\",  \"sensor_channel\":30, \"window_length\":200, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":10, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":10},#\n",
    "    # {\"dataset_name\":\"GesHome\",  \"sensor_channel\":9, \"window_length\":50,# \n",
    "    #  \"fft_window_length\":7, \"nb_classes\":18, \"fft_feature_dim\":20, #\n",
    "    #  \"S_number_sensors_type\":3,\"L_sensor_locations\":1},#\n",
    "    # {\"dataset_name\":\"lsign\",  \"sensor_channel\":15, \"window_length\":75, #\n",
    "    #  \"fft_window_length\":6, \"nb_classes\":36, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":5},#\n",
    "    # {\"dataset_name\":\"emgg\",  \"sensor_channel\":9, \"window_length\":200, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":7, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":3},#\n",
    "    # {\"dataset_name\":\"hhar\",  \"sensor_channel\":6, \"window_length\":200, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":6, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":2,\"L_sensor_locations\":1},#\n",
    "    {\"dataset_name\":\"mhealth\",  \"sensor_channel\":18, \"window_length\":128, #\n",
    "     \"fft_window_length\":8 , \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "     \"S_number_sensors_type\":3,\"L_sensor_locations\":2}#\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6760c4-2a79-433b-97e1-65ef7efcb2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4447b34b-7523-4c8e-ba34-4ce6427603db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      " self.nr_segment : 11\n",
      "-----------use fusion -----\n"
     ]
    }
   ],
   "source": [
    "results={}\n",
    "for data in data_dict:\n",
    "    #print(data)\n",
    "    result_data = {}\n",
    "    dataset_name=data[\"dataset_name\"]\n",
    "    sensor_channel=data[\"sensor_channel\"]\n",
    "    window_length=data[\"window_length\"]\n",
    "    fft_window_length=data[\"fft_window_length\"]\n",
    "    nb_classes=data[\"nb_classes\"]\n",
    "    fft_feature_dim=data[\"fft_feature_dim\"]\n",
    "    S_number_sensors_type=data[\"S_number_sensors_type\"]\n",
    "    L_sensor_locations=data[\"L_sensor_locations\"]\n",
    "    \n",
    "    \n",
    "    #  ----------------  build DCL model ---------------------\n",
    "\n",
    "    config = {  \"nb_conv_blocks\": 2,\n",
    "      \"nb_filters\": 64,\n",
    "      \"dilation\": 1,\n",
    "      \"batch_norm\": 0,\n",
    "      \"filter_width\": 7,\n",
    "      \"nb_layers_lstm\": 1,\n",
    "      \"drop_prob\": 0.2,\n",
    "      \"nb_units_lstm\": 128}\n",
    "    input_shape = (1, 1,window_length,sensor_channel)\n",
    "    model = DeepConvLSTM(input_shape, nb_classes, 1,config=config)\n",
    "    #x = torch.rand(1, 1, window_length, sensor_channel)\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "    result_data[\"dcl_macs\"] = macs\n",
    "    result_data[\"dcl_params\"] = params\n",
    "    # -------------- build DeepSense Model \n",
    "\n",
    "    if fft_feature_dim<=21:\n",
    "        kernel_size = 3\n",
    "    else:\n",
    "        kernel_size = 5\n",
    "    model = DeepSense(input_shape=(1, 1, window_length, sensor_channel),   fft_segments_length = int(fft_feature_dim/2),   \n",
    "                      k_number_sensors_group=S_number_sensors_type*L_sensor_locations, nb_classes= nb_classes, kernel_size_1=kernel_size)\n",
    "\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "    result_data[\"DeepSense_macs\"] = macs\n",
    "    result_data[\"DeepSense_params\"] = params\n",
    "\n",
    "    # ==============build  Globalfusion model \n",
    "\n",
    "    model = GlobalFusion(input_shape=(1, 1, window_length, sensor_channel),      fft_segments_length = int(fft_feature_dim/2),   \n",
    "                         S_number_sensors_type=S_number_sensors_type, \n",
    "                         L_sensor_locations= L_sensor_locations, nb_classes= nb_classes, kernel_size_1 = kernel_size)\n",
    "\n",
    "\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=True)\n",
    "\n",
    "    result_data[\"GlobalFusion_macs\"] = macs\n",
    "    result_data[\"GlobalFusion_params\"] = params\n",
    "\n",
    "    # ---------------- Build Attend\n",
    "    input_shape = (1, 1,window_length,sensor_channel)\n",
    "    config = {\"hidden_dim\": 128,\n",
    "      \"filter_num\": 64,\n",
    "      \"filter_size\": 5,\n",
    "      \"enc_num_layers\": 2,\n",
    "      \"dropout\": 0.5,\n",
    "      \"dropout_rnn\": 0.25,\n",
    "      \"dropout_cls\": 0.5,\n",
    "      \"activation\": \"ReLU\",\n",
    "      \"sa_div\": 1}\n",
    "    model = AttendDiscriminate(input_shape, nb_classes, config=config)\n",
    "    #x = torch.rand(1, 1, window_length, sensor_channel)\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"Attend_macs\"] = macs\n",
    "    result_data[\"Attend_params\"] = params\n",
    "    # ---------- Build ALAE ----------\n",
    "\n",
    "\n",
    "    model = ALAE_TAE(input_shape=(1, 1, window_length, sensor_channel),      \n",
    "                     nb_classes=nb_classes)\n",
    "\n",
    "    #x = torch.rand(1, 1, window_length, sensor_channel)\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"ALAE_macs\"] = macs\n",
    "    result_data[\"ALAE_params\"] = params\n",
    "    # ---------Build  IF Conv --------------\n",
    "\n",
    "\n",
    "    \n",
    "    if sensor_channel%9==0:\n",
    "        \n",
    "        fusion = True\n",
    "    else:\n",
    "        fusion = False\n",
    "    model = If_ConvTransformer_W(1, sensor_channel, 32, 5, 3, 2, 64, 1, 0.2, dataset_name, window_length, nb_classes, fusion = fusion)\n",
    "    \n",
    "    #input = torch.randn(1, 1,sensor_channel , window_length)\n",
    "\n",
    "    \n",
    "    \n",
    "    macs, params = get_model_complexity_info(model, (1,sensor_channel , window_length), as_strings=True,\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"IfConv_macs\"] = macs\n",
    "    result_data[\"IfConv_params\"] = params\n",
    "    # --------------- Build cross Attention -------------\n",
    "\n",
    "\n",
    "    # model = CrossAttn(\n",
    "    #     Ts_input_shape = (1,1,window_length,sensor_channel),\n",
    "    #     hidden_dim   = 24,\n",
    "    #     FFT_input_shape = (1,fft_feature_dim,fft_window_length,sensor_channel)\n",
    "    # )\n",
    "\n",
    "\n",
    "    # def prepare_input(resolution):\n",
    "    #     x1 = torch.FloatTensor(1, 1, window_length, sensor_channel)\n",
    "    #     x2 = torch.FloatTensor(1, fft_feature_dim, fft_window_length, sensor_channel)\n",
    "    #     return dict(x = [x1, x2])\n",
    "\n",
    "\n",
    "\n",
    "    # macs, params = get_model_complexity_info(model, \n",
    "    #                                          (1, window_length, sensor_channel), \n",
    "    #                                          input_constructor=prepare_input, \n",
    "    #                                          as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "    # result_data[\"CrossAttn_macs\"] = macs\n",
    "    # result_data[\"CrossAttn_params\"] = params\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # results[dataset_name] = result_data\n",
    "\n",
    "    # ----------------- build the Mixer Model -------------\n",
    "\n",
    "    config[\"fft_mixer_share_flag\"] = False\n",
    "    config[\"fft_mixer_temporal_flag\"]  = True\n",
    "    config[\"fft_mixer_FFT_flag\"]  = True\n",
    "    model  = FFTMIXER_HAR_Model(input_shape=(1,1, window_length, sensor_channel ), \n",
    "                                number_class = nb_classes,\n",
    "                                filter_num = 6,\n",
    "                                fft_mixer_segments_length = int(fft_feature_dim/2),\n",
    "                                expansion_factor = 0.3,\n",
    "                                fft_mixer_layer_nr = 2,\n",
    "                                fuse_early = False,\n",
    "                                temporal_merge= True,\n",
    "                                oration = 0.25,\n",
    "                                model_config = config)\n",
    "    \n",
    "    macs, params = get_model_complexity_info(model, (1,window_length , sensor_channel), as_strings=True,\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"Mixer_macs\"] = macs\n",
    "    result_data[\"Mixer_params\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff07bfb4-0758-4f1a-a168-c68962579474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft_feature_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5823a4-4826-42bc-a20a-6e6a33eb7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dcl_macs': '241.26 MMac',\n",
       " 'dcl_params': '744.65 k',\n",
       " 'DeepSense_macs': '45.11 MMac',\n",
       " 'DeepSense_params': '325.26 k',\n",
       " 'GlobalFusion_macs': '38.76 MMac',\n",
       " 'GlobalFusion_params': '333.64 k',\n",
       " 'Attend_macs': '226.53 MMac',\n",
       " 'Attend_params': '667.34 k',\n",
       " 'ALAE_macs': '190.41 MMac',\n",
       " 'ALAE_params': '413.13 k',\n",
       " 'IfConv_macs': '87.39 MMac',\n",
       " 'IfConv_params': '170.74 k',\n",
       " 'Mixer_macs': '1.06 MMac',\n",
       " 'Mixer_params': '87.89 k'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0547c1-4f85-49bd-af8b-9007e2c26f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d47e0-725e-4e09-a9ec-fe0ffa2e9c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f231ad-da65-4fb3-ae6d-686b7e0e441e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf7fd9-48fe-47b0-8a16-d05336af45d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3873639-873a-499f-ad8c-c64e5acb0629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599558c-ff76-4b55-a24b-0b7b3d653ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19d316-c84b-415f-be31-252cdffb337a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66214eb-30d0-4b1d-90b5-9f1602fe7d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pamap2': {'dcl_macs': '401.66 MMac',\n",
       "  'dcl_params': '745.43 k',\n",
       "  'DeepSense_macs': '90.83 MMac',\n",
       "  'DeepSense_params': '456.72 k',\n",
       "  'GlobalFusion_macs': '69.18 MMac',\n",
       "  'GlobalFusion_params': '355.41 k',\n",
       "  'Attend_macs': '369.22 MMac',\n",
       "  'Attend_params': '668.12 k',\n",
       "  'ALAE_macs': '297.52 MMac',\n",
       "  'ALAE_params': '413.52 k',\n",
       "  'IfConv_macs': '140.26 MMac',\n",
       "  'IfConv_params': '171.13 k',\n",
       "  'CrossAttn_macs': '19.38 MMac',\n",
       "  'CrossAttn_params': '124.88 k'},\n",
       " 'dsads': {'dcl_macs': '576.15 MMac',\n",
       "  'dcl_params': '1.63 M',\n",
       "  'DeepSense_macs': '106.97 MMac',\n",
       "  'DeepSense_params': '623.51 k',\n",
       "  'GlobalFusion_macs': '108.61 MMac',\n",
       "  'GlobalFusion_params': '431.89 k',\n",
       "  'Attend_macs': '543.88 MMac',\n",
       "  'Attend_params': '1.33 M',\n",
       "  'ALAE_macs': '454.44 MMac',\n",
       "  'ALAE_params': '855.96 k',\n",
       "  'IfConv_macs': '205.82 MMac',\n",
       "  'IfConv_params': '248.29 k',\n",
       "  'CrossAttn_macs': '29.12 MMac',\n",
       "  'CrossAttn_params': '187.8 k'},\n",
       " 'dg': {'dcl_macs': '124.16 MMac',\n",
       "  'dcl_params': '448.45 k',\n",
       "  'DeepSense_macs': '35.24 MMac',\n",
       "  'DeepSense_params': '323.65 k',\n",
       "  'GlobalFusion_macs': '18.99 MMac',\n",
       "  'GlobalFusion_params': '308.03 k',\n",
       "  'Attend_macs': '120.55 MMac',\n",
       "  'Attend_params': '444.87 k',\n",
       "  'ALAE_macs': '98.77 MMac',\n",
       "  'ALAE_params': '265.03 k',\n",
       "  'IfConv_macs': '51.03 MMac',\n",
       "  'IfConv_params': '144.39 k',\n",
       "  'CrossAttn_macs': '6.28 MMac',\n",
       "  'CrossAttn_params': '103.56 k'},\n",
       " 'rw': {'dcl_macs': '439.46 MMac',\n",
       "  'dcl_params': '1.63 M',\n",
       "  'DeepSense_macs': '57.77 MMac',\n",
       "  'DeepSense_params': '278.34 k',\n",
       "  'GlobalFusion_macs': '59.82 MMac',\n",
       "  'GlobalFusion_params': '373.19 k',\n",
       "  'Attend_macs': '421.76 MMac',\n",
       "  'Attend_params': '1.33 M',\n",
       "  'ALAE_macs': '363.55 MMac',\n",
       "  'ALAE_params': '855.24 k',\n",
       "  'IfConv_macs': '164.02 MMac',\n",
       "  'IfConv_params': '247.57 k',\n",
       "  'CrossAttn_macs': '23.06 MMac',\n",
       "  'CrossAttn_params': '185.64 k'},\n",
       " 'hapt': {'dcl_macs': '124.16 MMac',\n",
       "  'dcl_params': '449.74 k',\n",
       "  'DeepSense_macs': '35.24 MMac',\n",
       "  'DeepSense_params': '324.3 k',\n",
       "  'GlobalFusion_macs': '26.19 MMac',\n",
       "  'GlobalFusion_params': '313.16 k',\n",
       "  'Attend_macs': '120.55 MMac',\n",
       "  'Attend_params': '446.16 k',\n",
       "  'ALAE_macs': '98.77 MMac',\n",
       "  'ALAE_params': '265.68 k',\n",
       "  'IfConv_macs': '51.03 MMac',\n",
       "  'IfConv_params': '145.04 k',\n",
       "  'CrossAttn_macs': '6.28 MMac',\n",
       "  'CrossAttn_params': '103.56 k'},\n",
       " 'motionsense': {'dcl_macs': '163.19 MMac',\n",
       "  'dcl_params': '547.27 k',\n",
       "  'DeepSense_macs': '38.53 MMac',\n",
       "  'DeepSense_params': '324.23 k',\n",
       "  'GlobalFusion_macs': '33.97 MMac',\n",
       "  'GlobalFusion_params': '330.57 k',\n",
       "  'Attend_macs': '155.62 MMac',\n",
       "  'Attend_params': '519.11 k',\n",
       "  'ALAE_macs': '129.32 MMac',\n",
       "  'ALAE_params': '314.44 k',\n",
       "  'IfConv_macs': '51.0 MMac',\n",
       "  'IfConv_params': '143.78 k',\n",
       "  'CrossAttn_macs': '8.09 MMac',\n",
       "  'CrossAttn_params': '110.48 k'},\n",
       " 'skoda': {'dcl_macs': '661.47 MMac',\n",
       "  'dcl_params': '1.14 M',\n",
       "  'DeepSense_macs': '115.79 MMac',\n",
       "  'DeepSense_params': '457.48 k',\n",
       "  'GlobalFusion_macs': '94.11 MMac',\n",
       "  'GlobalFusion_params': '473.16 k',\n",
       "  'Attend_macs': '605.4 MMac',\n",
       "  'Attend_params': '962.0 k',\n",
       "  'ALAE_macs': '488.45 MMac',\n",
       "  'ALAE_params': '609.61 k',\n",
       "  'IfConv_macs': '169.95 MMac',\n",
       "  'IfConv_params': '180.91 k',\n",
       "  'CrossAttn_macs': '31.89 MMac',\n",
       "  'CrossAttn_params': '152.52 k'},\n",
       " 'GesHome': {'dcl_macs': '34.63 MMac',\n",
       "  'dcl_params': '450.51 k',\n",
       "  'DeepSense_macs': '22.8 MMac',\n",
       "  'DeepSense_params': '275.15 k',\n",
       "  'GlobalFusion_macs': '16.37 MMac',\n",
       "  'GlobalFusion_params': '294.23 k',\n",
       "  'Attend_macs': '38.19 MMac',\n",
       "  'Attend_params': '446.93 k',\n",
       "  'ALAE_macs': '38.58 MMac',\n",
       "  'ALAE_params': '266.07 k',\n",
       "  'IfConv_macs': '18.93 MMac',\n",
       "  'IfConv_params': '145.43 k',\n",
       "  'CrossAttn_macs': '2.14 MMac',\n",
       "  'CrossAttn_params': '102.7 k'},\n",
       " 'lsign': {'dcl_macs': '103.23 MMac',\n",
       "  'dcl_params': '649.44 k',\n",
       "  'DeepSense_macs': '50.75 MMac',\n",
       "  'DeepSense_params': '457.57 k',\n",
       "  'GlobalFusion_macs': '32.15 MMac',\n",
       "  'GlobalFusion_params': '378.21 k',\n",
       "  'Attend_macs': '102.4 MMac',\n",
       "  'Attend_params': '596.71 k',\n",
       "  'ALAE_macs': '93.67 MMac',\n",
       "  'ALAE_params': '365.54 k',\n",
       "  'IfConv_macs': '34.05 MMac',\n",
       "  'IfConv_params': '151.88 k',\n",
       "  'CrossAttn_macs': '5.53 MMac',\n",
       "  'CrossAttn_params': '117.96 k'},\n",
       " 'emgg': {'dcl_macs': '206.8 MMac',\n",
       "  'dcl_params': '449.1 k',\n",
       "  'DeepSense_macs': '72.11 MMac',\n",
       "  'DeepSense_params': '455.05 k',\n",
       "  'GlobalFusion_macs': '37.35 MMac',\n",
       "  'GlobalFusion_params': '327.88 k',\n",
       "  'Attend_macs': '196.57 MMac',\n",
       "  'Attend_params': '445.51 k',\n",
       "  'ALAE_macs': '154.33 MMac',\n",
       "  'ALAE_params': '265.35 k',\n",
       "  'IfConv_macs': '83.45 MMac',\n",
       "  'IfConv_params': '144.71 k',\n",
       "  'CrossAttn_macs': '10.43 MMac',\n",
       "  'CrossAttn_params': '104.14 k'},\n",
       " 'hhar': {'dcl_macs': '141.85 MMac',\n",
       "  'dcl_params': '350.66 k',\n",
       "  'DeepSense_macs': '65.87 MMac',\n",
       "  'DeepSense_params': '454.66 k',\n",
       "  'GlobalFusion_macs': '36.74 MMac',\n",
       "  'GlobalFusion_params': '306.76 k',\n",
       "  'Attend_macs': '139.88 MMac',\n",
       "  'Attend_params': '371.66 k',\n",
       "  'ALAE_macs': '106.6 MMac',\n",
       "  'ALAE_params': '216.13 k',\n",
       "  'IfConv_macs': '56.42 MMac',\n",
       "  'IfConv_params': '131.5 k',\n",
       "  'CrossAttn_macs': '7.52 MMac',\n",
       "  'CrossAttn_params': '97.23 k'},\n",
       " 'mhealth': {'dcl_macs': '241.26 MMac',\n",
       "  'dcl_params': '744.65 k',\n",
       "  'DeepSense_macs': '45.11 MMac',\n",
       "  'DeepSense_params': '325.26 k',\n",
       "  'GlobalFusion_macs': '38.76 MMac',\n",
       "  'GlobalFusion_params': '333.64 k',\n",
       "  'Attend_macs': '226.53 MMac',\n",
       "  'Attend_params': '667.34 k',\n",
       "  'ALAE_macs': '190.41 MMac',\n",
       "  'ALAE_params': '413.13 k',\n",
       "  'IfConv_macs': '87.39 MMac',\n",
       "  'IfConv_params': '170.74 k',\n",
       "  'CrossAttn_macs': '11.78 MMac',\n",
       "  'CrossAttn_params': '124.3 k'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cccb01-200a-4880-8a93-9c8f09bfea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(results).T.to_csv(\"flop2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c224296-27bf-49fb-bba7-15d85f9ef16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    if sensor_channel//9==0:\n",
    "        \n",
    "        fusion = True\n",
    "    else:\n",
    "        fusion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfe9dc10-eeed-4829-8374-6b83c8987ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops estimation was not finished successfully because ofthe following exception:\n",
      "<class 'RuntimeError'> : Given groups=1, weight of size [32, 32, 1, 5], expected input[1, 1, 6, 200] to have 32 channels, but got 1 channels instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kit/tm/px6680/miniconda3/envs/iswc23/lib/python3.9/site-packages/ptflops/pytorch_engine.py\", line 62, in get_flops_pytorch\n",
      "    _ = flops_model(batch)\n",
      "  File \"/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1071, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/pfs/data5/home/kit/tm/px6680/Conference/ISWC2023/I2S0W2C2_CFC/models/ifconv.py\", line 411, in forward\n",
      "    x = self.conv2(x)\n",
      "  File \"/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 139, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1071, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 443, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 439, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Given groups=1, weight of size [32, 32, 1, 5], expected input[1, 1, 6, 200] to have 32 channels, but got 1 channels instead\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m If_ConvTransformer_W(\u001b[38;5;241m1\u001b[39m, sensor_channel, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, dataset_name, window_length, nb_classes, fusion \u001b[38;5;241m=\u001b[39m fusion)\n\u001b[0;32m----> 3\u001b[0m macs, params \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_complexity_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msensor_channel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mprint_per_layer_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iswc23/lib/python3.9/site-packages/ptflops/flops_counter.py:42\u001b[0m, in \u001b[0;36mget_model_complexity_info\u001b[0;34m(model, input_res, print_per_layer_stat, as_strings, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks, backend, flops_units, param_units, output_precision)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrong backend name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_strings:\n\u001b[0;32m---> 42\u001b[0m     flops_string \u001b[38;5;241m=\u001b[39m \u001b[43mflops_to_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflops_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflops_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_precision\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     params_string \u001b[38;5;241m=\u001b[39m params_to_string(\n\u001b[1;32m     48\u001b[0m         params_count,\n\u001b[1;32m     49\u001b[0m         units\u001b[38;5;241m=\u001b[39mparam_units,\n\u001b[1;32m     50\u001b[0m         precision\u001b[38;5;241m=\u001b[39moutput_precision\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flops_string, params_string\n",
      "File \u001b[0;32m~/miniconda3/envs/iswc23/lib/python3.9/site-packages/ptflops/utils.py:12\u001b[0m, in \u001b[0;36mflops_to_string\u001b[0;34m(flops, units, precision)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflops_to_string\u001b[39m(flops, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m units \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mflops\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(flops \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10.\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m9\u001b[39m, precision)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m GMac\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m flops \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "model = If_ConvTransformer_W(1, sensor_channel, 32, 5, 3, 2, 64, 1, 0.2, dataset_name, window_length, nb_classes, fusion = fusion)\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1,sensor_channel , window_length), as_strings=True,\n",
    "                                       print_per_layer_stat=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10837b8d-ca88-4329-8cd0-f44626fd16c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       87.39 MMac\n",
      "Number of parameters:           170.74 k\n"
     ]
    }
   ],
   "source": [
    "# {\"dataset_name\":\"mhealth\",  \"sensor_channel\":18, \"window_length\":128, #\n",
    "#  \"fft_window_length\":8 , \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "#  \"S_number_sensors_type\":3,\"L_sensor_locations\":2}#\n",
    "\n",
    "batch_size =1\n",
    "feature_dim = 1\n",
    "sensor_channel = 18\n",
    "dataset_name = \"Opportunity\"\n",
    "window_length = 128\n",
    "nb_classes = 12\n",
    "\n",
    "if sensor_channel%9==0:\n",
    "    \n",
    "    fusion = True\n",
    "else:\n",
    "    fusion = False\n",
    "model = If_ConvTransformer_W(feature_dim, sensor_channel, 32, 5, 3, 2, 64, 1, 0.2, dataset_name, window_length, nb_classes, fusion = fusion)\n",
    "\n",
    "input = torch.randn(1, 1,sensor_channel , window_length)\n",
    "t = model(input)\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1,sensor_channel , window_length), as_strings=True,\n",
    "                                       print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f888187-d770-4c52-8108-a058747f7e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "Computational complexity:       11.78 MMac\n",
      "Number of parameters:           124.3 k \n"
     ]
    }
   ],
   "source": [
    "# {\"dataset_name\":\"mhealth\",  \"sensor_channel\":18, \"window_length\":128, #\n",
    "#  \"fft_window_length\":8 , \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "#  \"S_number_sensors_type\":3,\"L_sensor_locations\":2}#\n",
    "\n",
    "batch_size = 1\n",
    "window_length = 128\n",
    "sensor_channel = 18\n",
    "ft_window_length = 8\n",
    "ft_feature_dim = 32\n",
    "hidden_dim = 24\n",
    "\n",
    "model = CrossAttn(\n",
    "    Ts_input_shape = (batch_size,1,window_length,sensor_channel),\n",
    "    hidden_dim   = hidden_dim,\n",
    "    FFT_input_shape = (batch_size,ft_feature_dim,ft_window_length,sensor_channel)\n",
    ")\n",
    "\n",
    "x_ts = torch.rand(batch_size, 1, window_length, sensor_channel)\n",
    "x_ft = torch.rand(batch_size, ft_feature_dim, ft_window_length, sensor_channel)\n",
    "\n",
    "y1,y2 = model([x_ts, x_ft])\n",
    "\n",
    "\n",
    "def prepare_input(resolution):\n",
    "    x1 = torch.FloatTensor(1, 1, window_length, sensor_channel)\n",
    "    x2 = torch.FloatTensor(1, ft_feature_dim, ft_window_length, sensor_channel)\n",
    "    return dict(x = [x1, x2])\n",
    "\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, window_length, sensor_channel), input_constructor=prepare_input, as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d848e0e-8a6d-4974-9281-53cb42e49fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bff4f-0f92-4996-ad45-dcc54be859cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a43ac1-f7ec-4435-ad6e-570c9d900586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24defdf9-60e8-42ca-9f69-412cb43ca30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       124.16 MMac\n",
      "Number of parameters:           448.97 k\n"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "feature_dim = 1\n",
    "window_length = 128\n",
    "sensor_channel = 9\n",
    "\n",
    "input_shape = (batch_size, feature_dim,window_length,sensor_channel)\n",
    "num_class = 6\n",
    "filter_scaling_factor=1\n",
    "\n",
    "config = {  \"nb_conv_blocks\": 2,\n",
    "  \"nb_filters\": 64,\n",
    "  \"dilation\": 1,\n",
    "  \"batch_norm\": 0,\n",
    "  \"filter_width\": 7,\n",
    "  \"nb_layers_lstm\": 1,\n",
    "  \"drop_prob\": 0.2,\n",
    "  \"nb_units_lstm\": 128}\n",
    "\n",
    "model = DeepConvLSTM(input_shape, num_class, filter_scaling_factor,config=config)\n",
    "\n",
    "\n",
    "x = torch.rand(batch_size, feature_dim, window_length, sensor_channel)\n",
    "\n",
    "y =model(x)\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (feature_dim, window_length, sensor_channel), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f202bbcf-a418-4f18-9700-11acd5431052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       120.55 MMac\n",
      "Number of parameters:           445.38 k\n"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "feature_dim = 1\n",
    "window_length = 128\n",
    "sensor_channel = 9\n",
    "\n",
    "input_shape = (batch_size, feature_dim,window_length,sensor_channel)\n",
    "num_class = 6\n",
    "filter_scaling_factor=1\n",
    "\n",
    "config = {\"hidden_dim\": 128,\n",
    "  \"filter_num\": 64,\n",
    "  \"filter_size\": 5,\n",
    "  \"enc_num_layers\": 2,\n",
    "  \"dropout\": 0.5,\n",
    "  \"dropout_rnn\": 0.25,\n",
    "  \"dropout_cls\": 0.5,\n",
    "  \"activation\": \"ReLU\",\n",
    "  \"sa_div\": 1}\n",
    "\n",
    "model = AttendDiscriminate(input_shape, num_class, config=config)\n",
    "\n",
    "\n",
    "x = torch.rand(batch_size, feature_dim, window_length, sensor_channel)\n",
    "\n",
    "y =model(x)\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (feature_dim, window_length, sensor_channel), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b539bba-7717-4d05-aa83-10fef575d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "x torch.Size([1, 64, 128, 9])\n",
      "torch.Size([1, 64, 128, 9])\n",
      "Computational complexity:       98.77 MMac\n",
      "Number of parameters:           265.29 k\n"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "feature_dim = 1\n",
    "window_length = 128\n",
    "sensor_channel = 9\n",
    "filter_nr = 64\n",
    "nb_classes = 6\n",
    "\n",
    "model = ALAE_TAE(input_shape=(batch_size, feature_dim, window_length, sensor_channel),      nb_classes=nb_classes)\n",
    "\n",
    "x = torch.rand(batch_size, feature_dim, window_length, sensor_channel)\n",
    "\n",
    "y =model(x)\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (feature_dim, window_length, sensor_channel), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b3f1b-15c8-43b5-b5d7-74cf1542d6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8504ed-07e5-4d7b-b73d-d27f20cd65a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ada039-8cf1-4516-a85d-38d32535f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      "Computational complexity:       38.76 MMac\n",
      "Number of parameters:           333.64 k\n"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "fft_feature_dim = 32\n",
    "window_length = 128\n",
    "sensor_channel = 18\n",
    "S_number_sensors_type = 3\n",
    "fft_segments_length = 16\n",
    "nb_classes = 12\n",
    "L_sensor_locations =2\n",
    "model = GlobalFusion(\n",
    "    input_shape=(batch_size, fft_feature_dim, window_length, sensor_channel),      \n",
    "    S_number_sensors_type=S_number_sensors_type, \n",
    "    L_sensor_locations= L_sensor_locations, \n",
    "    fft_segments_length=fft_segments_length,\n",
    "    nb_classes= nb_classes)\n",
    "\n",
    "x = torch.rand(batch_size, 1, window_length, sensor_channel)\n",
    "\n",
    "y =model(x)\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, window_length, sensor_channel), as_strings=True, print_per_layer_stat=False, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a84c8f-7ede-4bfc-ad2a-520dfedecd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "    {\"dataset_name\":\"mhealth\",  \"sensor_channel\":18, \"window_length\":128, #\n",
    "     \"fft_window_length\":8 , \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "     \"S_number_sensors_type\":3,\"L_sensor_locations\":2}#\n",
    "]\n",
    " 'mhealth': {'dcl_macs': '241.26 MMac',\n",
    "  'dcl_params': '744.65 k',\n",
    "  'DeepSense_macs': '45.11 MMac',\n",
    "  'DeepSense_params': '325.26 k',\n",
    "  'GlobalFusion_macs': '38.76 MMac',\n",
    "  'GlobalFusion_params': '333.64 k',\n",
    "  'Attend_macs': '226.53 MMac',\n",
    "  'Attend_params': '667.34 k',\n",
    "  'ALAE_macs': '190.41 MMac',\n",
    "  'ALAE_params': '413.13 k',\n",
    "  'IfConv_macs': '87.39 MMac',\n",
    "  'IfConv_params': '170.74 k',\n",
    "  'CrossAttn_macs': '11.78 MMac',\n",
    "  'CrossAttn_params': '124.3 k'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7520b102-b73c-4b25-aa64-171fef3414c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       45.11 MMac\n",
      "Number of parameters:           325.26 k\n"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "fft_segments_length = 16\n",
    "window_length = 128\n",
    "sensor_channel = 18\n",
    "k_number_sensors_group = 6\n",
    "nb_classes =12\n",
    "model = DeepSense(\n",
    "    input_shape=(batch_size, 1, window_length, sensor_channel),  \n",
    "    k_number_sensors_group=k_number_sensors_group, \n",
    "    fft_segments_length = fft_segments_length,\n",
    "    nb_classes= nb_classes)\n",
    "\n",
    "x = torch.rand(batch_size, 1, window_length, sensor_channel)\n",
    "\n",
    "y =model(x)\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, window_length, sensor_channel), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3e2198-bd95-48a8-a4c3-6d2b334b9991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "Computational complexity:       11.03 MMac\n",
      "Number of parameters:           181.6 k \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "window_length = 128\n",
    "sensor_channel = 9\n",
    "ft_window_length = 10\n",
    "ft_feature_dim = 32\n",
    "hidden_dim = 32\n",
    "\n",
    "model = CrossAttn(\n",
    "    Ts_input_shape = (batch_size,1,window_length,sensor_channel),\n",
    "    hidden_dim   = hidden_dim,\n",
    "    FFT_input_shape = (batch_size,ft_feature_dim,ft_window_length,sensor_channel)\n",
    ")\n",
    "\n",
    "x_ts = torch.rand(batch_size, 1, window_length, sensor_channel)\n",
    "x_ft = torch.rand(batch_size, ft_feature_dim, ft_window_length, sensor_channel)\n",
    "\n",
    "y1,y2 = model([x_ts, x_ft])\n",
    "\n",
    "\n",
    "def prepare_input(resolution):\n",
    "    x1 = torch.FloatTensor(1, 1, window_length, sensor_channel)\n",
    "    x2 = torch.FloatTensor(1, ft_feature_dim, ft_window_length, sensor_channel)\n",
    "    return dict(x = [x1, x2])\n",
    "\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, window_length, sensor_channel), input_constructor=prepare_input, as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b789bc-49b4-418f-8c94-2824edc30b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       3.94 MMac\n",
      "Number of parameters:           40.69 k \n"
     ]
    }
   ],
   "source": [
    "input_length = 128\n",
    "c_in = 9\n",
    "num_classes = 12\n",
    "filter_num = 24\n",
    "model  = TinyHAR_Model((1,1, input_length, c_in ), \n",
    "                                         num_classes,\n",
    "                                         filter_num = filter_num,#config[\"filter_num\"],\n",
    "                                         cross_channel_interaction_type = \"attn\",    # attn  transformer  identity\n",
    "                                         cross_channel_aggregation_type = \"FC\",  # filter  naive  FC\n",
    "                                         temporal_info_interaction_type = \"lstm\",     # gru  lstm  attn  transformer  identity\n",
    "                                         temporal_info_aggregation_type = \"tnaive\")    # naive  filter  FC )\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, input_length, c_in), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e8b13-2a12-4aaa-8b6c-ffbec00779ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b10e0c2-88aa-485e-9861-25679d53d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 30\n",
      "150 30\n",
      "90 18\n",
      "90 18\n",
      "150 30\n",
      "150 30\n",
      "90 18\n",
      "90 18\n",
      "Computational complexity:       281.58 KMac\n",
      "Number of parameters:           31.24 k \n"
     ]
    }
   ],
   "source": [
    "input_length = 128\n",
    "c_in = 9\n",
    "num_classes = 12\n",
    "filter_num = 5\n",
    "fft_mixer_segments_length = 16\n",
    "\n",
    "\n",
    "model = FFTMIXER_HAR_Model(\n",
    "      input_shape=(1,1,input_length,c_in),\n",
    "      number_class=num_classes,\n",
    "      filter_num = filter_num,\n",
    "      fft_mixer_segments_length = fft_mixer_segments_length,\n",
    "      fft_mixer_share_flag=False,\n",
    "      fft_mixer_temporal_flag=True,\n",
    "      fft_mixer_FFT_flag=True,\n",
    "      fft_mixer_layer_nr=2,\n",
    "        expansion_factor = 0.2\n",
    ")\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, input_length, c_in), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5dc5bd-e13d-42e5-a5a6-7933bc797a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5184"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*9*8*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ce5e9f-a81f-433a-abb4-a9d274690554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*8*8*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bb722-d36a-4d64-9273-59214fbdfff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799b6ca-59e4-455d-873c-242ef4dc85af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f761933-b493-4ea4-be12-1c98e0e9d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "window_length = 128\n",
    "sensor_channel = 9\n",
    "ft_window_length = 10\n",
    "ft_feature_dim = 32\n",
    "hidden_dim = 32\n",
    "\n",
    "model = CrossAttn(\n",
    "    Ts_input_shape = (batch_size,1,window_length,sensor_channel),\n",
    "    hidden_dim   = hidden_dim,\n",
    "    FFT_input_shape = (batch_size,ft_feature_dim,ft_window_length,sensor_channel)\n",
    ")\n",
    "\n",
    "x_ts = torch.rand(batch_size, 1, window_length, sensor_channel)\n",
    "x_ft = torch.rand(batch_size, ft_feature_dim, ft_window_length, sensor_channel)\n",
    "\n",
    "y1,y2 = model([x_ts, x_ft])\n",
    "\n",
    "\n",
    "def prepare_input(resolution):\n",
    "    x1 = torch.FloatTensor(1, 1, window_length, sensor_channel)\n",
    "    x2 = torch.FloatTensor(1, ft_feature_dim, ft_window_length, sensor_channel)\n",
    "    return dict(x = [x1, x2])\n",
    "\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, window_length, sensor_channel), input_constructor=prepare_input, as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iswc23",
   "language": "python",
   "name": "iswc23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
