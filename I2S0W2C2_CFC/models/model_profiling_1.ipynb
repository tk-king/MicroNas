{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99158b9-9309-4fb1-964b-cd803f9960f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ptflops import get_model_complexity_info\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from CrossAttn import CrossAttn\n",
    "from deepSense import DeepSense\n",
    "from GlobalFusion import GlobalFusion\n",
    "from ALAE import ALAE_TAE\n",
    "from Attend import AttendDiscriminate\n",
    "from deepconvlstm import DeepConvLSTM\n",
    "from ifconv import If_ConvTransformer_W\n",
    "from TinyHAR import TinyHAR_Model\n",
    "from MixerMLP import FFTMIXER_HAR_Model\n",
    "from mcnn import MCNN\n",
    "from deepconvlstm_attn import DeepConvLSTM_ATTN\n",
    "from visionmixer import Vision_MIXER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d1625-fa12-4d93-87bb-4c50b4894512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fed596-813e-478b-a22f-0f4f62685d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1056,  0.0702,  0.1043, -0.1146, -0.0097, -0.0070,  0.0428, -0.0394,\n",
      "          0.0921, -0.0025, -0.0145, -0.0359]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "window_length = 128\n",
    "sensor_channel = 9\n",
    "filter_num = 32\n",
    "nb_classes = 12\n",
    "model  = TinyHAR_Model((1,1, window_length, sensor_channel ), \n",
    "                       nb_classes,\n",
    "                       filter_num = filter_num,#config[\"filter_num\"],\n",
    "                       cross_channel_interaction_type = \"attn\",    # attn  transformer  identity\n",
    "                       cross_channel_aggregation_type = \"FC\",  # filter  naive  FC\n",
    "                       temporal_info_interaction_type = \"lstm\",     # gru  lstm  attn  transformer  identity\n",
    "                       temporal_info_aggregation_type = \"tnaive\")    # naive  filter  FC )\n",
    "y = model(torch.randn((1,1, window_length, sensor_channel )))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9578f26-9704-4b85-9bc2-9ea7452fbae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c84a3d-0788-4724-8769-fc006f9bf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [\n",
    "    {\"dataset_name\":\"pamap2\",  \"sensor_channel\":18, \"window_length\":128, #\n",
    "     \"fft_window_length\":8, \"nb_classes\":18, \"fft_feature_dim\":32, #\n",
    "     \"S_number_sensors_type\":2,\"L_sensor_locations\":3},#\n",
    "    {\"dataset_name\":\"dsads\",  \"sensor_channel\":45, \"window_length\":125, #\n",
    "     \"fft_window_length\":5, \"nb_classes\":19, \"fft_feature_dim\":50, #\n",
    "     \"S_number_sensors_type\":3,\"L_sensor_locations\":5},#\n",
    "    {\"dataset_name\":\"dg\",  \"sensor_channel\":9, \"window_length\":64, #\n",
    "     \"fft_window_length\":4, \"nb_classes\":2, \"fft_feature_dim\":32, #\n",
    "     \"S_number_sensors_type\":1,\"L_sensor_locations\":3},#\n",
    "    # # {\"dataset_name\":\"rw\",  \"sensor_channel\":45, \"window_length\":100, #\n",
    "    # #  \"fft_window_length\":10, \"nb_classes\":8, \"fft_feature_dim\":20, #\n",
    "    # #  \"S_number_sensors_type\":3,\"L_sensor_locations\":5},#\n",
    "    {\"dataset_name\":\"hapt\",  \"sensor_channel\":6, \"window_length\":128, #\n",
    "     \"fft_window_length\":8, \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "     \"S_number_sensors_type\":2,\"L_sensor_locations\":1},#\n",
    "    {\"dataset_name\":\"motionsense\",  \"sensor_channel\":12, \"window_length\":128, #\n",
    "     \"fft_window_length\":8, \"nb_classes\":6, \"fft_feature_dim\":32, \n",
    "     \"S_number_sensors_type\":4,\"L_sensor_locations\":1},#\n",
    "    # {\"dataset_name\":\"skoda\",  \"sensor_channel\":30, \"window_length\":128, #\n",
    "    #  \"fft_window_length\":8, \"nb_classes\":10, \"fft_feature_dim\":32, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":10},#\n",
    "    # {\"dataset_name\":\"GesHome\",  \"sensor_channel\":9, \"window_length\":50,# \n",
    "    #  \"fft_window_length\":7, \"nb_classes\":18, \"fft_feature_dim\":20, #\n",
    "    #  \"S_number_sensors_type\":3,\"L_sensor_locations\":1},#\n",
    "    # {\"dataset_name\":\"lsign\",  \"sensor_channel\":15, \"window_length\":75, #\n",
    "    #  \"fft_window_length\":6, \"nb_classes\":36, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":5},#\n",
    "    # {\"dataset_name\":\"emgg\",  \"sensor_channel\":9, \"window_length\":200, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":7, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":1,\"L_sensor_locations\":3},#\n",
    "    # {\"dataset_name\":\"hhar\",  \"sensor_channel\":6, \"window_length\":200, #\n",
    "    #  \"fft_window_length\":10, \"nb_classes\":6, \"fft_feature_dim\":40, #\n",
    "    #  \"S_number_sensors_type\":2,\"L_sensor_locations\":1},#\n",
    "    {\"dataset_name\":\"mhealth\",  \"sensor_channel\":18, \"window_length\":128, #\n",
    "     \"fft_window_length\":8 , \"nb_classes\":12, \"fft_feature_dim\":32, #\n",
    "     \"S_number_sensors_type\":3,\"L_sensor_locations\":2}#\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2492836d-81cf-4b7f-b387-a597c02d9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcnn globalfusion deepsense bianda\n",
    "# dcl bian \n",
    "# qiyu baochi bubian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4447b34b-7523-4c8e-ba34-4ce6427603db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      "Warning: module GlobalFusion is treated as a zero-op.\n",
      "Warning: module GlobalFusion is treated as a zero-op.\n"
     ]
    }
   ],
   "source": [
    "results={}\n",
    "for data in data_dict:\n",
    "    #print(data)\n",
    "    result_data = {}\n",
    "    dataset_name=data[\"dataset_name\"]\n",
    "    sensor_channel=data[\"sensor_channel\"]\n",
    "    window_length=data[\"window_length\"]\n",
    "    fft_window_length=data[\"fft_window_length\"]\n",
    "    nb_classes=data[\"nb_classes\"]\n",
    "    fft_feature_dim=data[\"fft_feature_dim\"]\n",
    "    S_number_sensors_type=data[\"S_number_sensors_type\"]\n",
    "    L_sensor_locations=data[\"L_sensor_locations\"]\n",
    "    \n",
    "    #  ----------------  build MCNN model ---------------------\n",
    "\n",
    "\n",
    "    config = { \"nb_conv_blocks\": 2,\n",
    "              \"nb_filters\": 64,\n",
    "              \"dilation\": 1,\n",
    "              \"batch_norm\": 0,\n",
    "              \"filter_width\": 5,\n",
    "              \"drop_prob\": 0.25}\n",
    "    model  = MCNN((1,1, window_length, sensor_channel ), \n",
    "                   nb_classes,\n",
    "                  1,\n",
    "                  config)\n",
    "\n",
    "\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "    result_data[\"mcnn_macs\"] = macs\n",
    "    result_data[\"mcnn_params\"] = params\n",
    "    \n",
    "    #  ----------------  build DCL model ---------------------\n",
    "\n",
    "    config = {  \n",
    "        \"nb_conv_blocks\": 2,\n",
    "      \"nb_filters\": 64,\n",
    "      \"dilation\": 1,\n",
    "      \"batch_norm\": 0,\n",
    "      \"filter_width\": 7,\n",
    "      \"nb_layers_lstm\": 1,\n",
    "      \"drop_prob\": 0.2,\n",
    "      \"nb_units_lstm\": 128}\n",
    "\n",
    "\n",
    "\n",
    "    input_shape = (1, 1,window_length,sensor_channel)\n",
    "    model = DeepConvLSTM(input_shape, nb_classes, 1,config=config)\n",
    "    #x = torch.rand(1, 1, window_length, sensor_channel)\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "    result_data[\"dcl_macs\"] = macs\n",
    "    result_data[\"dcl_params\"] = params\n",
    "\n",
    "    # -----------------build DeepConvLSTM Attend -------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    config = {  \n",
    "        \"nb_conv_blocks\": 2,\n",
    "        \"nb_filters\": 64,\n",
    "        \"dilation\": 1,\n",
    "        \"batch_norm\": 0,\n",
    "        \"filter_width\": 5,\n",
    "        \"nb_layers_lstm\": 2,\n",
    "        \"drop_prob\": 0.5,\n",
    "        \"nb_units_lstm\": 128}\n",
    "    model  = DeepConvLSTM_ATTN((1,1, window_length, sensor_channel ), \n",
    "                               nb_classes,\n",
    "                               1,\n",
    "                               config)\n",
    "\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "    result_data[\"dcl_attn_macs\"] = macs\n",
    "    result_data[\"dcl_attn_params\"] = params\n",
    "    # -------------- build DeepSense Model \n",
    "\n",
    "    if fft_feature_dim<=21:\n",
    "        kernel_size = 3\n",
    "    else:\n",
    "        kernel_size = 5\n",
    "    model = DeepSense(input_shape=(1, 1, window_length, sensor_channel),   fft_segments_length = int(fft_feature_dim/2),   \n",
    "                      k_number_sensors_group=S_number_sensors_type*L_sensor_locations, nb_classes= nb_classes, kernel_size_1=kernel_size)\n",
    "\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "    result_data[\"DeepSense_macs\"] = macs\n",
    "    result_data[\"DeepSense_params\"] = params\n",
    "\n",
    "    # ==============build  Globalfusion model \n",
    "\n",
    "    model = GlobalFusion(input_shape=(1, 1, window_length, sensor_channel),      fft_segments_length = int(fft_feature_dim/2),   \n",
    "                         S_number_sensors_type=S_number_sensors_type, \n",
    "                         L_sensor_locations= L_sensor_locations, nb_classes= nb_classes, kernel_size_1 = kernel_size)\n",
    "\n",
    "\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=True)\n",
    "\n",
    "    result_data[\"GlobalFusion_macs\"] = macs\n",
    "    result_data[\"GlobalFusion_params\"] = params\n",
    "\n",
    "    # ---------------- Build Attend\n",
    "    input_shape = (1, 1,window_length,sensor_channel)\n",
    "    config = {\"hidden_dim\": 128,\n",
    "      \"filter_num\": 64,\n",
    "      \"filter_size\": 5,\n",
    "      \"enc_num_layers\": 2,\n",
    "      \"dropout\": 0.5,\n",
    "      \"dropout_rnn\": 0.25,\n",
    "      \"dropout_cls\": 0.5,\n",
    "      \"activation\": \"ReLU\",\n",
    "      \"sa_div\": 1}\n",
    "    model = AttendDiscriminate(input_shape, nb_classes, config=config)\n",
    "    #x = torch.rand(1, 1, window_length, sensor_channel)\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"Attend_macs\"] = macs\n",
    "    result_data[\"Attend_params\"] = params\n",
    "    # ---------- Build ALAE ----------\n",
    "\n",
    "\n",
    "    model = ALAE_TAE(input_shape=(1, 1, window_length, sensor_channel),      \n",
    "                     nb_classes=nb_classes)\n",
    "\n",
    "    #x = torch.rand(1, 1, window_length, sensor_channel)\n",
    "    macs, params = get_model_complexity_info(model, \n",
    "                                             (1, window_length, sensor_channel), \n",
    "                                             as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"ALAE_macs\"] = macs\n",
    "    result_data[\"ALAE_params\"] = params\n",
    "\n",
    "    \n",
    "    # ---------Build  IF Conv --------------\n",
    "\n",
    "\n",
    "    \n",
    "    if sensor_channel%9==0:\n",
    "        \n",
    "        fusion = True\n",
    "    else:\n",
    "        fusion = False\n",
    "    model = If_ConvTransformer_W(1, sensor_channel, 32, 5, 3, 2, 64, 1, 0.2, dataset_name, window_length, nb_classes, fusion = fusion)\n",
    "    \n",
    "    #input = torch.randn(1, 1,sensor_channel , window_length)\n",
    "\n",
    "    \n",
    "    \n",
    "    macs, params = get_model_complexity_info(model, (1,sensor_channel , window_length), as_strings=True,\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"IfConv_macs\"] = macs\n",
    "    result_data[\"IfConv_params\"] = params\n",
    "\n",
    "\n",
    "    # -------------------- Build TinyHAR ----------------\n",
    "\n",
    "    filter_num = 32\n",
    "    model  = TinyHAR_Model((1,1, window_length, sensor_channel ), \n",
    "                           nb_classes,\n",
    "                           filter_num = filter_num,#config[\"filter_num\"],\n",
    "                           cross_channel_interaction_type = \"attn\",    # attn  transformer  identity\n",
    "                           cross_channel_aggregation_type = \"FC\",  # filter  naive  FC\n",
    "                           temporal_info_interaction_type = \"lstm\",     # gru  lstm  attn  transformer  identity\n",
    "                           temporal_info_aggregation_type = \"tnaive\")    # naive  filter  FC )\n",
    "    \n",
    "    macs, params = get_model_complexity_info(\n",
    "        model, \n",
    "        (1, window_length, sensor_channel), \n",
    "        as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"tinyhar_macs\"] = macs\n",
    "    result_data[\"tinyhar_params\"] = params\n",
    "\n",
    "    results[dataset_name] = result_data\n",
    "\n",
    "\n",
    "    # -------------------- Build Visionmixer ----------------\n",
    "\n",
    "\n",
    "    model  = Vision_MIXER(\n",
    "                input_shape=(1,1, window_length, sensor_channel ), \n",
    "                patch_size = 3,\n",
    "                number_class= nb_classes) \n",
    "    \n",
    "    macs, params = get_model_complexity_info(\n",
    "        model, \n",
    "        (1, window_length, sensor_channel), \n",
    "        as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    result_data[\"Vision_MIXER_macs\"] = macs\n",
    "    result_data[\"Vision_MIXER_params\"] = params\n",
    "\n",
    "    results[dataset_name] = result_data\n",
    "    # --------------- Build cross Attention -------------\n",
    "\n",
    "\n",
    "    # model = CrossAttn(\n",
    "    #     Ts_input_shape = (1,1,window_length,sensor_channel),\n",
    "    #     hidden_dim   = 24,\n",
    "    #     FFT_input_shape = (1,fft_feature_dim,fft_window_length,sensor_channel)\n",
    "    # )\n",
    "\n",
    "\n",
    "    # def prepare_input(resolution):\n",
    "    #     x1 = torch.FloatTensor(1, 1, window_length, sensor_channel)\n",
    "    #     x2 = torch.FloatTensor(1, fft_feature_dim, fft_window_length, sensor_channel)\n",
    "    #     return dict(x = [x1, x2])\n",
    "\n",
    "\n",
    "\n",
    "    # macs, params = get_model_complexity_info(model, \n",
    "    #                                          (1, window_length, sensor_channel), \n",
    "    #                                          input_constructor=prepare_input, \n",
    "    #                                          as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "    # result_data[\"CrossAttn_macs\"] = macs\n",
    "    # result_data[\"CrossAttn_params\"] = params\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # results[dataset_name] = result_data\n",
    "\n",
    "    # ----------------- build the Mixer Model -------------\n",
    "\n",
    "    # config[\"fft_mixer_share_flag\"] = False\n",
    "    # config[\"fft_mixer_temporal_flag\"]  = True\n",
    "    # config[\"fft_mixer_FFT_flag\"]  = True\n",
    "    # model  = FFTMIXER_HAR_Model(input_shape=(1,1, window_length, sensor_channel ), \n",
    "    #                             number_class = nb_classes,\n",
    "    #                             filter_num = 6,\n",
    "    #                             fft_mixer_segments_length = int(fft_feature_dim/2),\n",
    "    #                             expansion_factor = 0.3,\n",
    "    #                             fft_mixer_layer_nr = 2,\n",
    "    #                             fuse_early = False,\n",
    "    #                             temporal_merge= True,\n",
    "    #                             oration = 0.25,\n",
    "    #                             model_config = config)\n",
    "    \n",
    "    # macs, params = get_model_complexity_info(model, (1,window_length , sensor_channel), as_strings=True,\n",
    "    #                                        print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "\n",
    "    # result_data[\"Mixer_macs\"] = macs\n",
    "    # result_data[\"Mixer_params\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892cca-3e76-4baf-8dd4-800c08598c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of Parameters : 387910 -------- Model Complexity : 105.31 MMac ------------ Number of parameters 2: 387.91 k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c46055-b39a-4110-8207-c8170306fe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcnn_macs': '58.25 MMac',\n",
       " 'mcnn_params': '637.26 k',\n",
       " 'dcl_macs': '241.26 MMac',\n",
       " 'dcl_params': '744.65 k',\n",
       " 'dcl_attn_macs': '74.97 MMac',\n",
       " 'dcl_attn_params': '868.68 k',\n",
       " 'DeepSense_macs': '45.11 MMac',\n",
       " 'DeepSense_params': '325.26 k',\n",
       " 'GlobalFusion_macs': '38.76 MMac',\n",
       " 'GlobalFusion_params': '333.64 k',\n",
       " 'Attend_macs': '226.53 MMac',\n",
       " 'Attend_params': '667.34 k',\n",
       " 'ALAE_macs': '190.41 MMac',\n",
       " 'ALAE_params': '413.13 k',\n",
       " 'IfConv_macs': '87.39 MMac',\n",
       " 'IfConv_params': '170.74 k',\n",
       " 'tinyhar_macs': '12.95 MMac',\n",
       " 'tinyhar_params': '90.09 k',\n",
       " 'Vision_MIXER_macs': '213.09 MMac',\n",
       " 'Vision_MIXER_params': '835.41 k'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"mhealth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5a0bbe-7658-4a81-a694-f35e1f428860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcnn_macs': '12.72 MMac',\n",
       " 'mcnn_params': '300.1 k',\n",
       " 'dcl_macs': '50.69 MMac',\n",
       " 'dcl_params': '448.45 k',\n",
       " 'dcl_attn_macs': '16.92 MMac',\n",
       " 'dcl_attn_params': '572.48 k',\n",
       " 'DeepSense_macs': '17.62 MMac',\n",
       " 'DeepSense_params': '323.65 k',\n",
       " 'GlobalFusion_macs': '9.49 MMac',\n",
       " 'GlobalFusion_params': '307.52 k',\n",
       " 'Attend_macs': '52.97 MMac',\n",
       " 'Attend_params': '444.87 k',\n",
       " 'ALAE_macs': '49.38 MMac',\n",
       " 'ALAE_params': '265.03 k',\n",
       " 'IfConv_macs': '24.46 MMac',\n",
       " 'IfConv_params': '144.39 k',\n",
       " 'tinyhar_macs': '2.86 MMac',\n",
       " 'tinyhar_params': '71.01 k',\n",
       " 'Vision_MIXER_macs': '54.58 MMac',\n",
       " 'Vision_MIXER_params': '709.0 k'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"dg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02cf85b-910f-4aae-8da5-af4a91cc9dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcnn_macs': '143.04 MMac',\n",
       " 'mcnn_params': '859.35 k',\n",
       " 'dcl_macs': '576.15 MMac',\n",
       " 'dcl_params': '1.63 M',\n",
       " 'dcl_attn_macs': '177.01 MMac',\n",
       " 'dcl_attn_params': '1.75 M',\n",
       " 'DeepSense_macs': '106.97 MMac',\n",
       " 'DeepSense_params': '623.51 k',\n",
       " 'GlobalFusion_macs': '108.61 MMac',\n",
       " 'GlobalFusion_params': '431.89 k',\n",
       " 'Attend_macs': '543.88 MMac',\n",
       " 'Attend_params': '1.33 M',\n",
       " 'ALAE_macs': '454.44 MMac',\n",
       " 'ALAE_params': '855.96 k',\n",
       " 'IfConv_macs': '205.82 MMac',\n",
       " 'IfConv_params': '248.29 k',\n",
       " 'tinyhar_macs': '32.68 MMac',\n",
       " 'tinyhar_params': '145.84 k',\n",
       " 'Vision_MIXER_macs': '520.22 MMac',\n",
       " 'Vision_MIXER_params': '1.08 M'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"dsads\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26fbb1c-c971-4ae6-b45d-b0a7eb38dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcnn_macs': '58.25 MMac',\n",
       " 'mcnn_params': '638.03 k',\n",
       " 'dcl_macs': '241.26 MMac',\n",
       " 'dcl_params': '745.43 k',\n",
       " 'dcl_attn_macs': '74.97 MMac',\n",
       " 'dcl_attn_params': '869.46 k',\n",
       " 'DeepSense_macs': '45.11 MMac',\n",
       " 'DeepSense_params': '325.65 k',\n",
       " 'GlobalFusion_macs': '35.17 MMac',\n",
       " 'GlobalFusion_params': '336.72 k',\n",
       " 'Attend_macs': '226.54 MMac',\n",
       " 'Attend_params': '668.12 k',\n",
       " 'ALAE_macs': '190.41 MMac',\n",
       " 'ALAE_params': '413.52 k',\n",
       " 'IfConv_macs': '87.4 MMac',\n",
       " 'IfConv_params': '171.13 k',\n",
       " 'tinyhar_macs': '12.95 MMac',\n",
       " 'tinyhar_params': '90.48 k',\n",
       " 'Vision_MIXER_macs': '213.1 MMac',\n",
       " 'Vision_MIXER_params': '836.96 k'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"pamap2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167979eb-0f2a-4868-a286-d005ca7072f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fcebe-6109-4416-a913-9ac823414214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c3f83-6144-46a9-b3fa-039be61a7256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0547c1-4f85-49bd-af8b-9007e2c26f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = {}\n",
    "for key1 in results.keys():\n",
    "    temp = results[key1]\n",
    "    df_re[\"{}_macs\".format(key1)] = []\n",
    "    df_re[\"{}_params\".format(key1)] = []\n",
    "    for key2 in temp.keys():\n",
    "        if \"macs\" in key2:\n",
    "            value = float(temp[key2].split(\" \")[0])\n",
    "            scale = temp[key2].split(\" \")[1]\n",
    "            if \"KM\" in scale:\n",
    "                value = value *1000\n",
    "            elif \"MM\" in scale:\n",
    "                value = value *1000000\n",
    "            elif \"GM\" in scale:\n",
    "                value = value *1000000000\n",
    "            else:\n",
    "                assert 1==0\n",
    "                \n",
    "            df_re[\"{}_macs\".format(key1)].append(value/1000000)\n",
    "        if \"params\" in key2:\n",
    "            value = float(temp[key2].split(\" \")[0])\n",
    "            scale = temp[key2].split(\" \")[1]\n",
    "            if \"k\" in scale:\n",
    "                value = value *1000\n",
    "            elif \"M\" in scale:\n",
    "                value = value *1000000\n",
    "            elif \"G\" in scale:\n",
    "                value = value *1000000000\n",
    "            else:\n",
    "                assert 1==0\n",
    "\n",
    "            \n",
    "            df_re[\"{}_params\".format(key1)].append(value/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e1055-8dea-4568-8669-efc124c5e82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66d47e0-725e-4e09-a9ec-fe0ffa2e9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df_re).T.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf7fd9-48fe-47b0-8a16-d05336af45d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3873639-873a-499f-ad8c-c64e5acb0629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599558c-ff76-4b55-a24b-0b7b3d653ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19d316-c84b-415f-be31-252cdffb337a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iswc23",
   "language": "python",
   "name": "iswc23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
