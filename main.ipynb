{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler set for device MicroNasMCU.NiclaSense\n",
      "[]\n",
      "Loaded LatencyPredictor with 0 samples\n"
     ]
    }
   ],
   "source": [
    "import dataloaders\n",
    "from dataloaders.dataloader import loadDataset\n",
    "import micronas\n",
    "from micronas import MicroNas, MicroNasMCU\n",
    "BASE_DATASET_FOLDER = r\"/Users/king/Github/timeseries-datasets/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "# train_data, vali_data, test_data = loadDataset(BASE_DATASET_FOLDER, \"ucihar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from micronas.config import Config\n",
    "\n",
    "class InMemoryDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        assert len(data_x) == len(data_y)\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]\n",
    "    \n",
    "    def save(self, name):\n",
    "        if not os.path.exists(os.path.join(BASE_DATASET_FOLDER, \"pickle\")):\n",
    "            os.makedirs(os.path.join(BASE_DATASET_FOLDER, \"pickle\"))\n",
    "\n",
    "        with open(os.path.join(BASE_DATASET_FOLDER, \"pickle\", name), \"wb\") as f:\n",
    "            pickle.dump({\"data_x\": self.data_x, \"data_y\": self.data_y}, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(name):\n",
    "        with open(os.path.join(BASE_DATASET_FOLDER, \"pickle\", name), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        data_x = torch.tensor(data[\"data_x\"], dtype=torch.float32)\n",
    "        data_y = torch.tensor(data[\"data_y\"], dtype=torch.long)\n",
    "        return InMemoryDataset(data_x, data_y)\n",
    "\n",
    "def extract_data(data_set):\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for x in tqdm(data_set):\n",
    "        train_data_x.append(x[0])\n",
    "        train_data_y.append(x[1])\n",
    "    train_data_x = np.array(train_data_x)\n",
    "    train_data_y = np.array(train_data_y)\n",
    "    return train_data_x, train_data_y\n",
    "\n",
    "\n",
    "# vali_data_x, vali_data_y = extract_data(vali_data)\n",
    "# vali_dataset = InMemoryDataset(vali_data_x, vali_data_y)\n",
    "# vali_dataset.save(\"ucihar_vali.pickle\")\n",
    "\n",
    "# train_data_x, train_data_y = extract_data(train_data)\n",
    "# train_dataset = InMemoryDataset(train_data_x, train_data_y)\n",
    "# train_dataset.save(\"ucihar_train.pickle\")\n",
    "\n",
    "vali_dataset = InMemoryDataset.load(\"ucihar_vali.pickle\")\n",
    "train_dataset = InMemoryDataset.load(\"ucihar_train.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(vali_dataset))[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "#Time_reduce:  3\n",
      "#Sensor_fusion:  1\n",
      "not included:  Dyn_Conv2D\n",
      "not included:  GlobalAveragePooling\n",
      "not included:  LogSoftMax\n",
      "not included:  Dyn_Conv2D\n",
      "not included:  GlobalAveragePooling\n",
      "not included:  LogSoftMax\n",
      "Epoch:  1  /  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:21<00:00,  5.08it/s, Loss=3.7, Loss_CE=3.71, Loss_LAT=0, Loss_MEM=-0.0045, eps=0.579, mean_lat=0, mean_mem=3200.228]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 109, Obj_avg: 3.906903e+00, Top1_avg: 38.903207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mNas = MicroNas(train_dataset, vali_dataset, vali_dataset, 6)\n",
    "mNas.search(MicroNasMCU.NUCLEOF446RE, latency_limit=None, memory_limit=30000, search_epochs=1, compute_unit=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micronas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
