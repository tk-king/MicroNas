{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloaders\n",
    "from dataloaders.dataloader import loadDataset\n",
    "import micronas\n",
    "from micronas import MicroNas, MicroNasMCU\n",
    "BASE_DATASET_FOLDER = r\"/Users/king/Github/timeseries-datasets/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "# train_data, vali_data, test_data = loadDataset(BASE_DATASET_FOLDER, \"ucihar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from micronas.config import Config\n",
    "\n",
    "class InMemoryDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        assert len(data_x) == len(data_y)\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]\n",
    "    \n",
    "    def save(self, name):\n",
    "        if not os.path.exists(os.path.join(BASE_DATASET_FOLDER, \"pickle\")):\n",
    "            os.makedirs(os.path.join(BASE_DATASET_FOLDER, \"pickle\"))\n",
    "\n",
    "        with open(os.path.join(BASE_DATASET_FOLDER, \"pickle\", name), \"wb\") as f:\n",
    "            pickle.dump({\"data_x\": self.data_x, \"data_y\": self.data_y}, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(name):\n",
    "        with open(os.path.join(BASE_DATASET_FOLDER, \"pickle\", name), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        data_x = torch.tensor(data[\"data_x\"], dtype=torch.float32)\n",
    "        data_y = torch.tensor(data[\"data_y\"], dtype=torch.long)\n",
    "        return InMemoryDataset(data_x, data_y)\n",
    "\n",
    "def extract_data(data_set):\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for x in tqdm(data_set):\n",
    "        train_data_x.append(x[0])\n",
    "        train_data_y.append(x[1])\n",
    "    train_data_x = np.array(train_data_x)\n",
    "    train_data_y = np.array(train_data_y)\n",
    "    return train_data_x, train_data_y\n",
    "\n",
    "\n",
    "# vali_data_x, vali_data_y = extract_data(vali_data)\n",
    "# vali_dataset = InMemoryDataset(vali_data_x, vali_data_y)\n",
    "# vali_dataset.save(\"ucihar_vali.pickle\")\n",
    "\n",
    "# train_data_x, train_data_y = extract_data(train_data)\n",
    "# train_dataset = InMemoryDataset(train_data_x, train_data_y)\n",
    "# train_dataset.save(\"ucihar_train.pickle\")\n",
    "\n",
    "vali_dataset = InMemoryDataset.load(\"ucihar_vali.pickle\")\n",
    "train_dataset = InMemoryDataset.load(\"ucihar_train.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(vali_dataset))[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mNas = MicroNas(train_dataset, vali_dataset, vali_dataset, 6)\n",
    "# mNas.search(MicroNasMCU.NUCLEOF446RE, latency_limit=None, memory_limit=30000, search_epochs=1, compute_unit=\"cpu\")\n",
    "from micronas.Nas.Networks.Pytorch.SearchNet import SearchNet\n",
    "from micronas.Nas.Networks.Pytorch.SearchModule import InferenceType\n",
    "import torch\n",
    "from micronas.Profiler.LatMemProfiler import set_ignore_latency, _lookUp\n",
    "\n",
    "net = SearchNet([128, 9], 6)\n",
    "\n",
    "set_ignore_latency(True)\n",
    "\n",
    "fake_input = torch.randn((1, 1, 128, 9))\n",
    "lat, mem = net(fake_input, inf_type=InferenceType.RANDOM)[1:]\n",
    "keras_model = net.getKeras(getPruned=True, inf_type=InferenceType.MAX_WEIGHT, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micronas import MicroNas, MicroNasMCU\n",
    "from micronas.Nas.Networks.Pytorch.SearchNet import SearchNet\n",
    "from micronas.Nas.SearchStrategy.DnasStrategy import DNasStrategy\n",
    "from micronas.Nas.SearchStrategy.RandomSearchStrategy import RandomSearchStrategy\n",
    "\n",
    "model = MicroNas(train_dataset, vali_dataset, vali_dataset, 6)\n",
    "\n",
    "input_dims = next(iter(train_dataset))[0].shape\n",
    "print(input_dims)\n",
    "search_space = SearchNet(input_dims=input_dims, num_classes=6)\n",
    "search_strategy = DNasStrategy(search_space)\n",
    "search_strategy = RandomSearchStrategy(search_space, arch_tries=3)\n",
    "\n",
    "model.compile(search_space, search_strategy)\n",
    "\n",
    "models = model.fit(MicroNasMCU.NUCLEOF446RE, latency_limit=None, memory_limit=30000, search_epochs=1, retrain_epochs=20, compute_unit=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micronas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
