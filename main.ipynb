{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler set for device MicroNasMCU.NiclaSense\n",
      "[]\n",
      "Loaded LatencyPredictor with 0 samples\n"
     ]
    }
   ],
   "source": [
    "import dataloaders\n",
    "from dataloaders.dataloader import loadDataset\n",
    "import micronas\n",
    "from micronas import MicroNas, MicroNasMCU\n",
    "BASE_DATASET_FOLDER = r\"/Users/king/Github/timeseries-datasets/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "# train_data, vali_data, test_data = loadDataset(BASE_DATASET_FOLDER, \"ucihar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from micronas.config import Config\n",
    "\n",
    "class InMemoryDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        assert len(data_x) == len(data_y)\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]\n",
    "    \n",
    "    def save(self, name):\n",
    "        if not os.path.exists(os.path.join(BASE_DATASET_FOLDER, \"pickle\")):\n",
    "            os.makedirs(os.path.join(BASE_DATASET_FOLDER, \"pickle\"))\n",
    "\n",
    "        with open(os.path.join(BASE_DATASET_FOLDER, \"pickle\", name), \"wb\") as f:\n",
    "            pickle.dump({\"data_x\": self.data_x, \"data_y\": self.data_y}, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(name):\n",
    "        with open(os.path.join(BASE_DATASET_FOLDER, \"pickle\", name), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        data_x = torch.tensor(data[\"data_x\"], dtype=torch.float32)\n",
    "        data_y = torch.tensor(data[\"data_y\"], dtype=torch.long)\n",
    "        return InMemoryDataset(data_x, data_y)\n",
    "\n",
    "def extract_data(data_set):\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for x in tqdm(data_set):\n",
    "        train_data_x.append(x[0])\n",
    "        train_data_y.append(x[1])\n",
    "    train_data_x = np.array(train_data_x)\n",
    "    train_data_y = np.array(train_data_y)\n",
    "    return train_data_x, train_data_y\n",
    "\n",
    "\n",
    "# vali_data_x, vali_data_y = extract_data(vali_data)\n",
    "# vali_dataset = InMemoryDataset(vali_data_x, vali_data_y)\n",
    "# vali_dataset.save(\"ucihar_vali.pickle\")\n",
    "\n",
    "# train_data_x, train_data_y = extract_data(train_data)\n",
    "# train_dataset = InMemoryDataset(train_data_x, train_data_y)\n",
    "# train_dataset.save(\"ucihar_train.pickle\")\n",
    "\n",
    "vali_dataset = InMemoryDataset.load(\"ucihar_vali.pickle\")\n",
    "train_dataset = InMemoryDataset.load(\"ucihar_train.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(vali_dataset))[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Time_reduce:  3\n",
      "#Sensor_fusion:  1\n"
     ]
    }
   ],
   "source": [
    "# mNas = MicroNas(train_dataset, vali_dataset, vali_dataset, 6)\n",
    "# mNas.search(MicroNasMCU.NUCLEOF446RE, latency_limit=None, memory_limit=30000, search_epochs=1, compute_unit=\"cpu\")\n",
    "from micronas.Nas.Networks.Pytorch.SearchNet import SearchNet\n",
    "from micronas.Nas.Networks.Pytorch.SearchModule import InferenceType\n",
    "import torch\n",
    "from micronas.Profiler.LatMemProfiler import set_ignore_latency, _lookUp\n",
    "\n",
    "net = SearchNet([128, 9], 6)\n",
    "\n",
    "set_ignore_latency(True)\n",
    "\n",
    "fake_input = torch.randn((1, 1, 128, 9))\n",
    "lat, mem = net(fake_input, inf_type=InferenceType.RANDOM)[1:]\n",
    "keras_model = net.getKeras(getPruned=True, inf_type=InferenceType.MAX_WEIGHT, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 9])\n",
      "#Time_reduce:  3\n",
      "#Sensor_fusion:  1\n",
      "not included:  Dyn_Conv2D\n",
      "not included:  GlobalAveragePooling\n",
      "not included:  LogSoftMax\n",
      "Weights_op_softmax:  None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m search_strategy \u001b[38;5;241m=\u001b[39m RandomSearchStrategy(search_space)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(search_space, search_strategy)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMicroNasMCU\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUCLEOF446RE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatency_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/main.py:60\u001b[0m, in \u001b[0;36mMicroNas.fit\u001b[0;34m(self, target_mcu, latency_limit, memory_limit, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m dataset_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# ts_len, num_sensors = dataset_shape[1:3]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# self.search_net = SearchNet([ts_len, num_sensors], self.num_classes).to(Config.compute_unit)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# searcher = ArchSearcher(self.search_net)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvali_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatency_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_limit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/Nas/SearchStrategy/RandomSearchStrategy.py:8\u001b[0m, in \u001b[0;36mRandomSearchStrategy.search\u001b[0;34m(self, train_dataloader, vali_dataloader, search_epochs, latency_limit, memory_limit, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_dataloader, vali_dataloader, search_epochs, latency_limit, memory_limit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 8\u001b[0m   keras_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetKeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43minf_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInferenceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRANDOM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetPruned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28mprint\u001b[39m(keras_net)\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/Nas/Networks/Pytorch/SearchNet.py:92\u001b[0m, in \u001b[0;36mSearchNet.getKeras\u001b[0;34m(self, batch_size, getPruned, inf_type)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetKeras\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, getPruned\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inf_type\u001b[38;5;241m=\u001b[39mInferenceType\u001b[38;5;241m.\u001b[39mNORMAL):\n\u001b[1;32m     91\u001b[0m     k_input \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput([\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dims, \u001b[38;5;241m1\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m---> 92\u001b[0m     k_time_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_time_reduce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetKeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetPruned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgetPruned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     k_ch_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ch_reduce\u001b[38;5;241m.\u001b[39mgetKeras(k_time_out, getPruned\u001b[38;5;241m=\u001b[39mgetPruned, inf_type\u001b[38;5;241m=\u001b[39minf_type)\n\u001b[1;32m     94\u001b[0m     k_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cls_stack\u001b[38;5;241m.\u001b[39mgetKeras(k_ch_out, getPruned\u001b[38;5;241m=\u001b[39mgetPruned)\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/Nas/Layers/Pytorch/Sequential.py:62\u001b[0m, in \u001b[0;36mSequential.getKeras\u001b[0;34m(self, x, getPruned, inf_type)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetKeras\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, getPruned, inf_type\u001b[38;5;241m=\u001b[39mInferenceType\u001b[38;5;241m.\u001b[39mNORMAL):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[0;32m---> 62\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetKeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetPruned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgetPruned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/Nas/Networks/Pytorch/SearchNet.py:304\u001b[0m, in \u001b[0;36mReduceTime.getKeras\u001b[0;34m(self, x, getPruned, inf_type)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inf_type \u001b[38;5;241m==\u001b[39m inf_type\u001b[38;5;241m.\u001b[39mMAX_WEIGHT:\n\u001b[1;32m    302\u001b[0m     weight_channels_softmax \u001b[38;5;241m=\u001b[39m weight_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_channels, \u001b[38;5;241m1e-9\u001b[39m, gumbel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 304\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_choice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetKeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetPruned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgetPruned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_channels_softmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m res \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m)(res)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/Nas/Layers/Pytorch/ChooseNParallel.py:170\u001b[0m, in \u001b[0;36mChooseNParallel.getKeras\u001b[0;34m(self, x, getPruned, weights, inf_type)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetKeras\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, getPruned, weights, inf_type):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getPruned:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getKeras_pruned\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getKeras_not_pruned(x, weights, inf_type)\n",
      "File \u001b[0;32m~/Github/MicroNas/micronas/Nas/Layers/Pytorch/ChooseNParallel.py:151\u001b[0m, in \u001b[0;36mChooseNParallel._getKeras_pruned\u001b[0;34m(self, x, weights, inf_type)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights_op_softmax: \u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_op_softmax)\n\u001b[1;32m    150\u001b[0m last_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweights_op_softmax\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxIdx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmaxIdx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetKeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetPruned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from micronas import MicroNas, MicroNasMCU\n",
    "from micronas.Nas.Networks.Pytorch.SearchNet import SearchNet\n",
    "from micronas.Nas.SearchStrategy.DnasStrategy import DNasStrategy\n",
    "from micronas.Nas.SearchStrategy.RandomSearchStrategy import RandomSearchStrategy\n",
    "\n",
    "model = MicroNas(train_dataset, vali_dataset, vali_dataset, 6)\n",
    "\n",
    "input_dims = next(iter(train_dataset))[0].shape\n",
    "print(input_dims)\n",
    "search_space = SearchNet(input_dims=input_dims, num_classes=6)\n",
    "search_strategy = DNasStrategy(search_space)\n",
    "search_strategy = RandomSearchStrategy(search_space)\n",
    "\n",
    "model.compile(search_space, search_strategy)\n",
    "\n",
    "model.fit(MicroNasMCU.NUCLEOF446RE, latency_limit=None, memory_limit=30000, search_epochs=1, compute_unit=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micronas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
